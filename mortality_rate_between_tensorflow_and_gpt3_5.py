# -*- coding: utf-8 -*-
"""mortality rate between tensorflow and gpt3.5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXKJi7iQQpibNk98SibLnTA1QT6krGUW
"""

!pip install -q tensorflow tensorflow-hub

!pip install -q openai pandas scikit-learn tensorflow matplotlib

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Load dataset
df = pd.read_excel("/content/sample_data/BoneCancer_200_Project (4).xlsx", sheet_name='Sheet1')
features = ['Primary Site', 'Site recode - rare tumors']
target = 'SEER cause-specific death classification'

# Encode categorical columns
encoder = LabelEncoder()
for col in features + [target]:
    df[col] = encoder.fit_transform(df[col].astype(str))

X = df[features]
y = df[target]

# Store all accuracies and histories
all_accuracies = []
all_histories = []

# Run model training and evaluation 100 times
for run in range(100):
    # Split and scale data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=run)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Build model
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train model
    history = model.fit(
        X_train_scaled, y_train,
        epochs=200,
        batch_size=16,
        validation_data=(X_test_scaled, y_test),
        verbose=0
    )

    # Store final val accuracy
    final_val_acc = history.history['val_accuracy'][-1]
    all_accuracies.append(final_val_acc)
    all_histories.append(history)

# Identify best and worst runs
sorted_indices = np.argsort(all_accuracies)[::-1]
best_index = sorted_indices[0]
worst_index = sorted_indices[-1]

print(f"Best Performing Run (#{best_index}): Accuracy = {all_accuracies[best_index]:.2f}")
print(f"Worst Performing Run (#{worst_index}): Accuracy = {all_accuracies[worst_index]:.2f}\n")

# Output top 5 performing runs
print("Top 5 Performing Runs:")
for i in range(5):
    idx = sorted_indices[i]
    print(f"Run #{idx}: Accuracy = {all_accuracies[idx]:.2f}")

# Plot performance trend over 100 runs
plt.figure(figsize=(12, 5))
plt.plot(all_accuracies, marker='o', label='Validation Accuracy')
plt.title("Validation Accuracy over 100 Runs")
plt.xlabel("Run Number")
plt.ylabel("Final Validation Accuracy")
plt.grid(True)
plt.axhline(y=all_accuracies[best_index], color='green', linestyle='--', label="Best")
plt.axhline(y=all_accuracies[worst_index], color='red', linestyle='--', label="Worst")
plt.legend()
plt.savefig("/mnt/data/validation_accuracy_over_100_runs.png")  # Save the plot
plt.show()



import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Step 1: Check the label classes
print("Unique values in y_test:", np.unique(y_test))

# Step 2: Get model predictions
y_pred_probs = model.predict(X_test)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()

# Step 3: Filter only binary-labeled samples (0 or 1)
binary_mask = y_test <= 1  # keep only class 0 and 1
y_test_binary = y_test[binary_mask]
y_pred_binary = y_pred[binary_mask]

# Step 4: Evaluate only on binary-labeled data
accuracy = accuracy_score(y_test_binary, y_pred_binary)
precision = precision_score(y_test_binary, y_pred_binary)
recall = recall_score(y_test_binary, y_pred_binary)
f1 = f1_score(y_test_binary, y_pred_binary)

# Step 5: Display the results
print("🧠 Binary Mortality Prediction Performance:")
print(f"Accuracy : {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall   : {recall:.2f}")
print(f"F1 Score : {f1:.2f}")

!pip install openai==0.28

import openai

openai.api_key = "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"

from sklearn.preprocessing import LabelEncoder

features = ['Primary Site', 'Site recode - rare tumors']
target = 'SEER cause-specific death classification'

encoders = {}

for col in features + [target]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    encoders[col] = le

!pip install -q tqdm

import time
import time
from tqdm.notebook import tqdm
import pandas as pd

import time
import os
import pandas as pd
from tqdm.notebook import tqdm
from google.colab import files

# Make a clean copy of your test set
X_test_original = X_test.copy()

# Store GPT-3.5 results
gpt_35_responses = []

print("🧠 Starting GPT-3.5 mortality predictions...")

for idx, row in tqdm(X_test.iterrows(), total=len(X_test)):
    # Decode feature values
    primary_site = encoders['Primary Site'].inverse_transform([row['Primary Site']])[0]
    rare_tumor = encoders['Site recode - rare tumors'].inverse_transform([row['Site recode - rare tumors']])[0]

    # Make prediction with GPT-3.5
    print(f"\n[{idx+1}/{len(X_test)}] GPT-3.5 - {primary_site} | {rare_tumor}")
    result = ask_gpt("gpt-3.5-turbo", primary_site, rare_tumor)
    gpt_35_responses.append(result)
    time.sleep(20)  # prevent hitting rate limit

# Assign predictions after loop
X_test_original['GPT-3.5'] = gpt_35_responses

# Save to CSV
X_test_original.to_csv("gpt_predictions.csv", index=False)

# Download CSV file
if os.path.exists("gpt_predictions.csv"):
    print("📁 Downloading gpt_predictions.csv...")
    files.download("gpt_predictions.csv")
else:
    print("❌ File not found for download.")

from sklearn.metrics import accuracy_score, classification_report

# True labels
y_true = y_test.to_numpy()

# GPT-3.5 predictions (ensure int type)
y_pred_gpt3 = X_test_original['GPT-3.5'].astype(int).to_numpy()

# TensorFlow predictions (already defined as y_pred_tf)
print(" TensorFlow Model Accuracy:", accuracy_score(y_true, y_pred_tf))
print(" GPT-3.5 Accuracy:", accuracy_score(y_true, y_pred_gpt3))

# Classification report for GPT-3.5
print("\n GPT-3.5 Classification Report:\n")
print(classification_report(y_true, y_pred_gpt3))

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

y_pred_tf = y_pred_tf.astype(int)
y_pred_gpt3 = X_test_original['GPT-3.5'].astype(int).to_numpy()

metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']

tf_scores = [
    accuracy_score(y_true, y_pred_tf),
    precision_score(y_true, y_pred_tf, average='weighted'),
    recall_score(y_true, y_pred_tf, average='weighted'),
    f1_score(y_true, y_pred_tf, average='weighted')
]

gpt3_scores = [
    accuracy_score(y_true, y_pred_gpt3),
    precision_score(y_true, y_pred_gpt3, average='weighted'),
    recall_score(y_true, y_pred_gpt3, average='weighted'),
    f1_score(y_true, y_pred_gpt3, average='weighted')
]

x = np.arange(len(metrics))
width = 0.35

plt.figure(figsize=(10, 6))
plt.bar(x - width/2, tf_scores, width, label='TensorFlow')
plt.bar(x + width/2, gpt3_scores, width, label='GPT-3.5')

plt.ylabel('Score')
plt.title('Performance Comparison: TensorFlow vs GPT-3.5')
plt.xticks(x, metrics)
plt.ylim(0, 1.1)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()